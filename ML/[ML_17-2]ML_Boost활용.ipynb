{
 "cells": [
  {
   "cell_type": "raw",
   "id": "86d258fd",
   "metadata": {},
   "source": [
    "<< (1) 파이썬 래퍼 XGBoost : xgboost ( DMatrix 객체 사용, train()/predict() 함수)    >>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25c40880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.3\n"
     ]
    }
   ],
   "source": [
    "#!pip install xgboost\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "\n",
    "print(xgb.__version__) # 2.0.3\n",
    "# 설치하면 같이 설치된다. 파이썬버전이랑 사이킷버전이랑, 가능한 사이킷버전꺼 쓰자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a3c1cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1a0f4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터로드\n",
    "dataset = load_breast_cancer()\n",
    "X_features = dataset.data\n",
    "y_label  = dataset.target\n",
    "\n",
    "cancer_df  = pd.DataFrame(data=X_features , columns = dataset.feature_names)\n",
    "cancer_df['target'] = y_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c61df03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "1    357\n",
       "0    212\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.target_names\n",
    "cancer_df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d31dfce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(455, 30) (114, 30) <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#데이터 분할  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features ,y_label, test_size =0.2, random_state =0 )\n",
    "\n",
    "print(X_train.shape, X_test.shape , type(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7b297ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'xgboost.core.DMatrix'> <class 'xgboost.core.DMatrix'>\n"
     ]
    }
   ],
   "source": [
    "##xgb 객체를 사용해서 연동하자. DMatrix 객체 사용, train()/predict()\n",
    "dtrain = xgb.DMatrix(data = X_train , label = y_train) \n",
    "dtest  = xgb.DMatrix(data = X_test, label = y_test)\n",
    "print(type(dtrain), type(dtest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08e6f720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.57667\teval-logloss:0.59996\n",
      "[1]\ttrain-logloss:0.51052\teval-logloss:0.53202\n",
      "[2]\ttrain-logloss:0.45640\teval-logloss:0.47599\n",
      "[3]\ttrain-logloss:0.40937\teval-logloss:0.43172\n",
      "[4]\ttrain-logloss:0.36944\teval-logloss:0.39426\n",
      "[5]\ttrain-logloss:0.33604\teval-logloss:0.36241\n",
      "[6]\ttrain-logloss:0.30733\teval-logloss:0.33102\n",
      "[7]\ttrain-logloss:0.27969\teval-logloss:0.30311\n",
      "[8]\ttrain-logloss:0.25514\teval-logloss:0.27903\n",
      "[9]\ttrain-logloss:0.23461\teval-logloss:0.25987\n",
      "[10]\ttrain-logloss:0.21538\teval-logloss:0.24220\n",
      "[11]\ttrain-logloss:0.19856\teval-logloss:0.22582\n",
      "[12]\ttrain-logloss:0.18383\teval-logloss:0.21112\n",
      "[13]\ttrain-logloss:0.17096\teval-logloss:0.19923\n",
      "[14]\ttrain-logloss:0.15849\teval-logloss:0.18557\n",
      "[15]\ttrain-logloss:0.14823\teval-logloss:0.17514\n",
      "[16]\ttrain-logloss:0.13902\teval-logloss:0.16465\n",
      "[17]\ttrain-logloss:0.13039\teval-logloss:0.15796\n",
      "[18]\ttrain-logloss:0.12166\teval-logloss:0.15020\n",
      "[19]\ttrain-logloss:0.11437\teval-logloss:0.14272\n",
      "[20]\ttrain-logloss:0.10793\teval-logloss:0.13722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python311\\site-packages\\xgboost\\core.py:160: UserWarning: [17:02:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"early_stoppings\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21]\ttrain-logloss:0.10127\teval-logloss:0.13219\n",
      "[22]\ttrain-logloss:0.09587\teval-logloss:0.12667\n",
      "[23]\ttrain-logloss:0.09080\teval-logloss:0.12251\n",
      "[24]\ttrain-logloss:0.08589\teval-logloss:0.11807\n",
      "[25]\ttrain-logloss:0.08131\teval-logloss:0.11375\n",
      "[26]\ttrain-logloss:0.07736\teval-logloss:0.10942\n",
      "[27]\ttrain-logloss:0.07372\teval-logloss:0.10503\n",
      "[28]\ttrain-logloss:0.07023\teval-logloss:0.10080\n",
      "[29]\ttrain-logloss:0.06712\teval-logloss:0.09842\n",
      "[30]\ttrain-logloss:0.06417\teval-logloss:0.09625\n",
      "[31]\ttrain-logloss:0.06131\teval-logloss:0.09241\n",
      "[32]\ttrain-logloss:0.05852\teval-logloss:0.08909\n",
      "[33]\ttrain-logloss:0.05607\teval-logloss:0.08695\n",
      "[34]\ttrain-logloss:0.05416\teval-logloss:0.08441\n",
      "[35]\ttrain-logloss:0.05200\teval-logloss:0.08208\n",
      "[36]\ttrain-logloss:0.04928\teval-logloss:0.08041\n",
      "[37]\ttrain-logloss:0.04738\teval-logloss:0.07901\n",
      "[38]\ttrain-logloss:0.04564\teval-logloss:0.07755\n",
      "[39]\ttrain-logloss:0.04384\teval-logloss:0.07583\n",
      "[40]\ttrain-logloss:0.04204\teval-logloss:0.07407\n",
      "[41]\ttrain-logloss:0.04046\teval-logloss:0.07263\n",
      "[42]\ttrain-logloss:0.03877\teval-logloss:0.07149\n",
      "[43]\ttrain-logloss:0.03742\teval-logloss:0.07004\n",
      "[44]\ttrain-logloss:0.03608\teval-logloss:0.06906\n",
      "[45]\ttrain-logloss:0.03498\teval-logloss:0.06790\n",
      "[46]\ttrain-logloss:0.03392\teval-logloss:0.06713\n",
      "[47]\ttrain-logloss:0.03262\teval-logloss:0.06614\n",
      "[48]\ttrain-logloss:0.03158\teval-logloss:0.06592\n",
      "[49]\ttrain-logloss:0.03053\teval-logloss:0.06575\n",
      "[50]\ttrain-logloss:0.02946\teval-logloss:0.06495\n",
      "[51]\ttrain-logloss:0.02859\teval-logloss:0.06501\n",
      "[52]\ttrain-logloss:0.02771\teval-logloss:0.06443\n",
      "[53]\ttrain-logloss:0.02692\teval-logloss:0.06386\n",
      "[54]\ttrain-logloss:0.02619\teval-logloss:0.06379\n",
      "[55]\ttrain-logloss:0.02555\teval-logloss:0.06333\n",
      "[56]\ttrain-logloss:0.02491\teval-logloss:0.06234\n",
      "[57]\ttrain-logloss:0.02426\teval-logloss:0.06244\n",
      "[58]\ttrain-logloss:0.02363\teval-logloss:0.06123\n",
      "[59]\ttrain-logloss:0.02294\teval-logloss:0.06084\n",
      "[60]\ttrain-logloss:0.02248\teval-logloss:0.06107\n",
      "[61]\ttrain-logloss:0.02194\teval-logloss:0.06114\n",
      "[62]\ttrain-logloss:0.02136\teval-logloss:0.06059\n",
      "[63]\ttrain-logloss:0.02091\teval-logloss:0.06046\n",
      "[64]\ttrain-logloss:0.02042\teval-logloss:0.06005\n",
      "[65]\ttrain-logloss:0.02005\teval-logloss:0.05992\n",
      "[66]\ttrain-logloss:0.01971\teval-logloss:0.05988\n",
      "[67]\ttrain-logloss:0.01927\teval-logloss:0.06001\n",
      "[68]\ttrain-logloss:0.01893\teval-logloss:0.05970\n",
      "[69]\ttrain-logloss:0.01853\teval-logloss:0.05994\n",
      "[70]\ttrain-logloss:0.01813\teval-logloss:0.05962\n",
      "[71]\ttrain-logloss:0.01784\teval-logloss:0.05925\n",
      "[72]\ttrain-logloss:0.01753\teval-logloss:0.05931\n",
      "[73]\ttrain-logloss:0.01732\teval-logloss:0.05919\n",
      "[74]\ttrain-logloss:0.01704\teval-logloss:0.05911\n",
      "[75]\ttrain-logloss:0.01676\teval-logloss:0.05871\n",
      "[76]\ttrain-logloss:0.01645\teval-logloss:0.05889\n",
      "[77]\ttrain-logloss:0.01614\teval-logloss:0.05859\n",
      "[78]\ttrain-logloss:0.01587\teval-logloss:0.05905\n",
      "[79]\ttrain-logloss:0.01554\teval-logloss:0.05856\n",
      "[80]\ttrain-logloss:0.01530\teval-logloss:0.05861\n",
      "[81]\ttrain-logloss:0.01514\teval-logloss:0.05846\n",
      "[82]\ttrain-logloss:0.01485\teval-logloss:0.05730\n",
      "[83]\ttrain-logloss:0.01462\teval-logloss:0.05755\n",
      "[84]\ttrain-logloss:0.01436\teval-logloss:0.05786\n",
      "[85]\ttrain-logloss:0.01421\teval-logloss:0.05772\n",
      "[86]\ttrain-logloss:0.01394\teval-logloss:0.05708\n",
      "[87]\ttrain-logloss:0.01381\teval-logloss:0.05696\n",
      "[88]\ttrain-logloss:0.01367\teval-logloss:0.05684\n",
      "[89]\ttrain-logloss:0.01351\teval-logloss:0.05697\n",
      "[90]\ttrain-logloss:0.01338\teval-logloss:0.05750\n",
      "[91]\ttrain-logloss:0.01325\teval-logloss:0.05740\n",
      "[92]\ttrain-logloss:0.01312\teval-logloss:0.05676\n",
      "[93]\ttrain-logloss:0.01301\teval-logloss:0.05664\n",
      "[94]\ttrain-logloss:0.01285\teval-logloss:0.05673\n",
      "[95]\ttrain-logloss:0.01273\teval-logloss:0.05724\n",
      "[96]\ttrain-logloss:0.01262\teval-logloss:0.05728\n",
      "[97]\ttrain-logloss:0.01241\teval-logloss:0.05708\n",
      "[98]\ttrain-logloss:0.01230\teval-logloss:0.05647\n",
      "[99]\ttrain-logloss:0.01208\teval-logloss:0.05564\n",
      "[100]\ttrain-logloss:0.01198\teval-logloss:0.05522\n",
      "[101]\ttrain-logloss:0.01187\teval-logloss:0.05533\n",
      "[102]\ttrain-logloss:0.01176\teval-logloss:0.05532\n",
      "[103]\ttrain-logloss:0.01166\teval-logloss:0.05580\n",
      "[104]\ttrain-logloss:0.01157\teval-logloss:0.05573\n",
      "[105]\ttrain-logloss:0.01148\teval-logloss:0.05611\n",
      "[106]\ttrain-logloss:0.01139\teval-logloss:0.05555\n",
      "[107]\ttrain-logloss:0.01125\teval-logloss:0.05517\n",
      "[108]\ttrain-logloss:0.01117\teval-logloss:0.05486\n",
      "[109]\ttrain-logloss:0.01108\teval-logloss:0.05499\n",
      "[110]\ttrain-logloss:0.01099\teval-logloss:0.05507\n",
      "[111]\ttrain-logloss:0.01093\teval-logloss:0.05516\n",
      "[112]\ttrain-logloss:0.01078\teval-logloss:0.05458\n",
      "[113]\ttrain-logloss:0.01069\teval-logloss:0.05471\n",
      "[114]\ttrain-logloss:0.01062\teval-logloss:0.05466\n",
      "[115]\ttrain-logloss:0.01054\teval-logloss:0.05416\n",
      "[116]\ttrain-logloss:0.01043\teval-logloss:0.05385\n",
      "[117]\ttrain-logloss:0.01036\teval-logloss:0.05381\n",
      "[118]\ttrain-logloss:0.01028\teval-logloss:0.05333\n",
      "[119]\ttrain-logloss:0.01018\teval-logloss:0.05305\n",
      "[120]\ttrain-logloss:0.01010\teval-logloss:0.05277\n",
      "[121]\ttrain-logloss:0.01002\teval-logloss:0.05304\n",
      "[122]\ttrain-logloss:0.00995\teval-logloss:0.05289\n",
      "[123]\ttrain-logloss:0.00989\teval-logloss:0.05286\n",
      "[124]\ttrain-logloss:0.00983\teval-logloss:0.05276\n",
      "[125]\ttrain-logloss:0.00976\teval-logloss:0.05314\n",
      "[126]\ttrain-logloss:0.00969\teval-logloss:0.05350\n",
      "[127]\ttrain-logloss:0.00963\teval-logloss:0.05305\n",
      "[128]\ttrain-logloss:0.00954\teval-logloss:0.05271\n",
      "[129]\ttrain-logloss:0.00947\teval-logloss:0.05243\n",
      "[130]\ttrain-logloss:0.00941\teval-logloss:0.05218\n",
      "[131]\ttrain-logloss:0.00935\teval-logloss:0.05232\n",
      "[132]\ttrain-logloss:0.00929\teval-logloss:0.05266\n",
      "[133]\ttrain-logloss:0.00924\teval-logloss:0.05257\n",
      "[134]\ttrain-logloss:0.00918\teval-logloss:0.05230\n",
      "[135]\ttrain-logloss:0.00913\teval-logloss:0.05188\n",
      "[136]\ttrain-logloss:0.00910\teval-logloss:0.05187\n",
      "[137]\ttrain-logloss:0.00903\teval-logloss:0.05223\n",
      "[138]\ttrain-logloss:0.00898\teval-logloss:0.05212\n",
      "[139]\ttrain-logloss:0.00893\teval-logloss:0.05172\n",
      "[140]\ttrain-logloss:0.00891\teval-logloss:0.05172\n",
      "[141]\ttrain-logloss:0.00886\teval-logloss:0.05189\n",
      "[142]\ttrain-logloss:0.00880\teval-logloss:0.05224\n",
      "[143]\ttrain-logloss:0.00875\teval-logloss:0.05258\n",
      "[144]\ttrain-logloss:0.00872\teval-logloss:0.05245\n",
      "[145]\ttrain-logloss:0.00866\teval-logloss:0.05219\n",
      "[146]\ttrain-logloss:0.00862\teval-logloss:0.05181\n",
      "[147]\ttrain-logloss:0.00859\teval-logloss:0.05180\n",
      "[148]\ttrain-logloss:0.00857\teval-logloss:0.05193\n",
      "[149]\ttrain-logloss:0.00855\teval-logloss:0.05173\n",
      "[150]\ttrain-logloss:0.00850\teval-logloss:0.05205\n",
      "[151]\ttrain-logloss:0.00845\teval-logloss:0.05168\n",
      "[152]\ttrain-logloss:0.00843\teval-logloss:0.05167\n",
      "[153]\ttrain-logloss:0.00841\teval-logloss:0.05183\n",
      "[154]\ttrain-logloss:0.00836\teval-logloss:0.05147\n",
      "[155]\ttrain-logloss:0.00834\teval-logloss:0.05147\n",
      "[156]\ttrain-logloss:0.00832\teval-logloss:0.05149\n",
      "[157]\ttrain-logloss:0.00830\teval-logloss:0.05164\n",
      "[158]\ttrain-logloss:0.00825\teval-logloss:0.05195\n",
      "[159]\ttrain-logloss:0.00821\teval-logloss:0.05160\n",
      "[160]\ttrain-logloss:0.00819\teval-logloss:0.05136\n",
      "[161]\ttrain-logloss:0.00813\teval-logloss:0.05105\n",
      "[162]\ttrain-logloss:0.00811\teval-logloss:0.05120\n",
      "[163]\ttrain-logloss:0.00807\teval-logloss:0.05085\n",
      "[164]\ttrain-logloss:0.00803\teval-logloss:0.05094\n",
      "[165]\ttrain-logloss:0.00801\teval-logloss:0.05076\n",
      "[166]\ttrain-logloss:0.00798\teval-logloss:0.05076\n",
      "[167]\ttrain-logloss:0.00797\teval-logloss:0.05091\n",
      "[168]\ttrain-logloss:0.00792\teval-logloss:0.05059\n",
      "[169]\ttrain-logloss:0.00790\teval-logloss:0.05042\n",
      "[170]\ttrain-logloss:0.00785\teval-logloss:0.05051\n",
      "[171]\ttrain-logloss:0.00784\teval-logloss:0.05056\n",
      "[172]\ttrain-logloss:0.00782\teval-logloss:0.05049\n",
      "[173]\ttrain-logloss:0.00776\teval-logloss:0.05055\n",
      "[174]\ttrain-logloss:0.00774\teval-logloss:0.05058\n",
      "[175]\ttrain-logloss:0.00773\teval-logloss:0.05042\n",
      "[176]\ttrain-logloss:0.00769\teval-logloss:0.05052\n",
      "[177]\ttrain-logloss:0.00767\teval-logloss:0.05066\n",
      "[178]\ttrain-logloss:0.00764\teval-logloss:0.05080\n",
      "[179]\ttrain-logloss:0.00762\teval-logloss:0.05072\n",
      "[180]\ttrain-logloss:0.00761\teval-logloss:0.05072\n",
      "[181]\ttrain-logloss:0.00757\teval-logloss:0.05080\n",
      "[182]\ttrain-logloss:0.00756\teval-logloss:0.05083\n",
      "[183]\ttrain-logloss:0.00754\teval-logloss:0.05097\n",
      "[184]\ttrain-logloss:0.00753\teval-logloss:0.05089\n",
      "[185]\ttrain-logloss:0.00750\teval-logloss:0.05090\n",
      "[186]\ttrain-logloss:0.00748\teval-logloss:0.05104\n",
      "[187]\ttrain-logloss:0.00747\teval-logloss:0.05089\n",
      "[188]\ttrain-logloss:0.00745\teval-logloss:0.05089\n",
      "[189]\ttrain-logloss:0.00742\teval-logloss:0.05097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[190]\ttrain-logloss:0.00741\teval-logloss:0.05101\n",
      "[191]\ttrain-logloss:0.00739\teval-logloss:0.05105\n",
      "[192]\ttrain-logloss:0.00738\teval-logloss:0.05110\n",
      "[193]\ttrain-logloss:0.00735\teval-logloss:0.05111\n",
      "[194]\ttrain-logloss:0.00734\teval-logloss:0.05124\n",
      "[195]\ttrain-logloss:0.00732\teval-logloss:0.05117\n",
      "[196]\ttrain-logloss:0.00731\teval-logloss:0.05118\n",
      "[197]\ttrain-logloss:0.00730\teval-logloss:0.05103\n",
      "[198]\ttrain-logloss:0.00728\teval-logloss:0.05116\n",
      "[199]\ttrain-logloss:0.00727\teval-logloss:0.05121\n",
      "[200]\ttrain-logloss:0.00725\teval-logloss:0.05123\n",
      "[201]\ttrain-logloss:0.00723\teval-logloss:0.05138\n",
      "[202]\ttrain-logloss:0.00722\teval-logloss:0.05152\n",
      "[203]\ttrain-logloss:0.00720\teval-logloss:0.05146\n",
      "[204]\ttrain-logloss:0.00719\teval-logloss:0.05146\n",
      "[205]\ttrain-logloss:0.00718\teval-logloss:0.05159\n",
      "[206]\ttrain-logloss:0.00716\teval-logloss:0.05145\n",
      "[207]\ttrain-logloss:0.00715\teval-logloss:0.05159\n",
      "[208]\ttrain-logloss:0.00714\teval-logloss:0.05173\n",
      "[209]\ttrain-logloss:0.00713\teval-logloss:0.05162\n",
      "[210]\ttrain-logloss:0.00711\teval-logloss:0.05176\n",
      "[211]\ttrain-logloss:0.00710\teval-logloss:0.05181\n",
      "[212]\ttrain-logloss:0.00709\teval-logloss:0.05186\n",
      "[213]\ttrain-logloss:0.00708\teval-logloss:0.05191\n",
      "[214]\ttrain-logloss:0.00706\teval-logloss:0.05165\n",
      "[215]\ttrain-logloss:0.00705\teval-logloss:0.05166\n",
      "[216]\ttrain-logloss:0.00704\teval-logloss:0.05152\n",
      "[217]\ttrain-logloss:0.00703\teval-logloss:0.05170\n",
      "[218]\ttrain-logloss:0.00701\teval-logloss:0.05164\n",
      "[219]\ttrain-logloss:0.00700\teval-logloss:0.05165\n",
      "[220]\ttrain-logloss:0.00699\teval-logloss:0.05141\n",
      "[221]\ttrain-logloss:0.00698\teval-logloss:0.05145\n",
      "[222]\ttrain-logloss:0.00697\teval-logloss:0.05159\n",
      "[223]\ttrain-logloss:0.00695\teval-logloss:0.05135\n",
      "[224]\ttrain-logloss:0.00694\teval-logloss:0.05137\n",
      "[225]\ttrain-logloss:0.00693\teval-logloss:0.05151\n",
      "[226]\ttrain-logloss:0.00692\teval-logloss:0.05145\n",
      "[227]\ttrain-logloss:0.00691\teval-logloss:0.05125\n",
      "[228]\ttrain-logloss:0.00690\teval-logloss:0.05129\n",
      "[229]\ttrain-logloss:0.00688\teval-logloss:0.05106\n",
      "[230]\ttrain-logloss:0.00687\teval-logloss:0.05107\n",
      "[231]\ttrain-logloss:0.00686\teval-logloss:0.05120\n",
      "[232]\ttrain-logloss:0.00685\teval-logloss:0.05124\n",
      "[233]\ttrain-logloss:0.00684\teval-logloss:0.05101\n",
      "[234]\ttrain-logloss:0.00683\teval-logloss:0.05103\n",
      "[235]\ttrain-logloss:0.00682\teval-logloss:0.05091\n",
      "[236]\ttrain-logloss:0.00681\teval-logloss:0.05069\n",
      "[237]\ttrain-logloss:0.00680\teval-logloss:0.05070\n",
      "[238]\ttrain-logloss:0.00678\teval-logloss:0.05083\n",
      "[239]\ttrain-logloss:0.00677\teval-logloss:0.05087\n",
      "[240]\ttrain-logloss:0.00676\teval-logloss:0.05068\n",
      "[241]\ttrain-logloss:0.00675\teval-logloss:0.05073\n",
      "[242]\ttrain-logloss:0.00674\teval-logloss:0.05085\n",
      "[243]\ttrain-logloss:0.00673\teval-logloss:0.05063\n",
      "[244]\ttrain-logloss:0.00672\teval-logloss:0.05065\n",
      "[245]\ttrain-logloss:0.00671\teval-logloss:0.05054\n",
      "[246]\ttrain-logloss:0.00670\teval-logloss:0.05059\n",
      "[247]\ttrain-logloss:0.00669\teval-logloss:0.05037\n",
      "[248]\ttrain-logloss:0.00668\teval-logloss:0.05038\n",
      "[249]\ttrain-logloss:0.00667\teval-logloss:0.05033\n",
      "[250]\ttrain-logloss:0.00666\teval-logloss:0.05047\n",
      "[251]\ttrain-logloss:0.00665\teval-logloss:0.05028\n",
      "[252]\ttrain-logloss:0.00664\teval-logloss:0.05032\n",
      "[253]\ttrain-logloss:0.00663\teval-logloss:0.05038\n",
      "[254]\ttrain-logloss:0.00662\teval-logloss:0.05039\n",
      "[255]\ttrain-logloss:0.00661\teval-logloss:0.05018\n",
      "[256]\ttrain-logloss:0.00660\teval-logloss:0.04999\n",
      "[257]\ttrain-logloss:0.00659\teval-logloss:0.05012\n",
      "[258]\ttrain-logloss:0.00658\teval-logloss:0.05001\n",
      "[259]\ttrain-logloss:0.00657\teval-logloss:0.05003\n",
      "[260]\ttrain-logloss:0.00656\teval-logloss:0.04982\n",
      "[261]\ttrain-logloss:0.00655\teval-logloss:0.04988\n",
      "[262]\ttrain-logloss:0.00654\teval-logloss:0.05000\n",
      "[263]\ttrain-logloss:0.00653\teval-logloss:0.04981\n",
      "[264]\ttrain-logloss:0.00652\teval-logloss:0.04983\n",
      "[265]\ttrain-logloss:0.00651\teval-logloss:0.04987\n",
      "[266]\ttrain-logloss:0.00650\teval-logloss:0.04967\n",
      "[267]\ttrain-logloss:0.00650\teval-logloss:0.04969\n",
      "[268]\ttrain-logloss:0.00649\teval-logloss:0.04959\n",
      "[269]\ttrain-logloss:0.00648\teval-logloss:0.04971\n",
      "[270]\ttrain-logloss:0.00647\teval-logloss:0.04976\n",
      "[271]\ttrain-logloss:0.00646\teval-logloss:0.04956\n",
      "[272]\ttrain-logloss:0.00645\teval-logloss:0.04937\n",
      "[273]\ttrain-logloss:0.00644\teval-logloss:0.04940\n",
      "[274]\ttrain-logloss:0.00643\teval-logloss:0.04920\n",
      "[275]\ttrain-logloss:0.00642\teval-logloss:0.04926\n",
      "[276]\ttrain-logloss:0.00641\teval-logloss:0.04931\n",
      "[277]\ttrain-logloss:0.00640\teval-logloss:0.04913\n",
      "[278]\ttrain-logloss:0.00639\teval-logloss:0.04915\n",
      "[279]\ttrain-logloss:0.00639\teval-logloss:0.04911\n",
      "[280]\ttrain-logloss:0.00638\teval-logloss:0.04922\n",
      "[281]\ttrain-logloss:0.00637\teval-logloss:0.04902\n",
      "[282]\ttrain-logloss:0.00636\teval-logloss:0.04884\n",
      "[283]\ttrain-logloss:0.00635\teval-logloss:0.04897\n",
      "[284]\ttrain-logloss:0.00634\teval-logloss:0.04901\n",
      "[285]\ttrain-logloss:0.00633\teval-logloss:0.04904\n",
      "[286]\ttrain-logloss:0.00632\teval-logloss:0.04888\n",
      "[287]\ttrain-logloss:0.00632\teval-logloss:0.04890\n",
      "[288]\ttrain-logloss:0.00631\teval-logloss:0.04881\n",
      "[289]\ttrain-logloss:0.00630\teval-logloss:0.04886\n",
      "[290]\ttrain-logloss:0.00629\teval-logloss:0.04869\n",
      "[291]\ttrain-logloss:0.00628\teval-logloss:0.04872\n",
      "[292]\ttrain-logloss:0.00627\teval-logloss:0.04853\n",
      "[293]\ttrain-logloss:0.00627\teval-logloss:0.04836\n",
      "[294]\ttrain-logloss:0.00626\teval-logloss:0.04848\n",
      "[295]\ttrain-logloss:0.00625\teval-logloss:0.04853\n",
      "[296]\ttrain-logloss:0.00624\teval-logloss:0.04843\n",
      "[297]\ttrain-logloss:0.00623\teval-logloss:0.04846\n",
      "[298]\ttrain-logloss:0.00622\teval-logloss:0.04827\n",
      "[299]\ttrain-logloss:0.00622\teval-logloss:0.04838\n",
      "[300]\ttrain-logloss:0.00621\teval-logloss:0.04821\n",
      "[301]\ttrain-logloss:0.00620\teval-logloss:0.04833\n",
      "[302]\ttrain-logloss:0.00619\teval-logloss:0.04829\n",
      "[303]\ttrain-logloss:0.00618\teval-logloss:0.04835\n",
      "[304]\ttrain-logloss:0.00618\teval-logloss:0.04818\n",
      "[305]\ttrain-logloss:0.00617\teval-logloss:0.04820\n",
      "[306]\ttrain-logloss:0.00616\teval-logloss:0.04825\n",
      "[307]\ttrain-logloss:0.00615\teval-logloss:0.04816\n",
      "[308]\ttrain-logloss:0.00615\teval-logloss:0.04820\n",
      "[309]\ttrain-logloss:0.00614\teval-logloss:0.04801\n",
      "[310]\ttrain-logloss:0.00613\teval-logloss:0.04804\n",
      "[311]\ttrain-logloss:0.00612\teval-logloss:0.04815\n",
      "[312]\ttrain-logloss:0.00611\teval-logloss:0.04798\n",
      "[313]\ttrain-logloss:0.00611\teval-logloss:0.04793\n",
      "[314]\ttrain-logloss:0.00610\teval-logloss:0.04797\n",
      "[315]\ttrain-logloss:0.00609\teval-logloss:0.04789\n",
      "[316]\ttrain-logloss:0.00608\teval-logloss:0.04773\n",
      "[317]\ttrain-logloss:0.00608\teval-logloss:0.04764\n",
      "[318]\ttrain-logloss:0.00607\teval-logloss:0.04777\n",
      "[319]\ttrain-logloss:0.00606\teval-logloss:0.04781\n",
      "[320]\ttrain-logloss:0.00605\teval-logloss:0.04787\n",
      "[321]\ttrain-logloss:0.00605\teval-logloss:0.04769\n",
      "[322]\ttrain-logloss:0.00604\teval-logloss:0.04772\n",
      "[323]\ttrain-logloss:0.00603\teval-logloss:0.04756\n",
      "[324]\ttrain-logloss:0.00602\teval-logloss:0.04759\n",
      "[325]\ttrain-logloss:0.00602\teval-logloss:0.04756\n",
      "[326]\ttrain-logloss:0.00601\teval-logloss:0.04767\n",
      "[327]\ttrain-logloss:0.00600\teval-logloss:0.04750\n",
      "[328]\ttrain-logloss:0.00600\teval-logloss:0.04733\n",
      "[329]\ttrain-logloss:0.00599\teval-logloss:0.04743\n",
      "[330]\ttrain-logloss:0.00598\teval-logloss:0.04739\n",
      "[331]\ttrain-logloss:0.00597\teval-logloss:0.04742\n",
      "[332]\ttrain-logloss:0.00597\teval-logloss:0.04734\n",
      "[333]\ttrain-logloss:0.00596\teval-logloss:0.04740\n",
      "[334]\ttrain-logloss:0.00595\teval-logloss:0.04742\n",
      "[335]\ttrain-logloss:0.00595\teval-logloss:0.04725\n",
      "[336]\ttrain-logloss:0.00594\teval-logloss:0.04729\n",
      "[337]\ttrain-logloss:0.00593\teval-logloss:0.04722\n",
      "[338]\ttrain-logloss:0.00592\teval-logloss:0.04726\n",
      "[339]\ttrain-logloss:0.00592\teval-logloss:0.04710\n",
      "[340]\ttrain-logloss:0.00591\teval-logloss:0.04706\n",
      "[341]\ttrain-logloss:0.00590\teval-logloss:0.04717\n",
      "[342]\ttrain-logloss:0.00590\teval-logloss:0.04702\n",
      "[343]\ttrain-logloss:0.00589\teval-logloss:0.04694\n",
      "[344]\ttrain-logloss:0.00588\teval-logloss:0.04703\n",
      "[345]\ttrain-logloss:0.00588\teval-logloss:0.04707\n",
      "[346]\ttrain-logloss:0.00587\teval-logloss:0.04690\n",
      "[347]\ttrain-logloss:0.00586\teval-logloss:0.04694\n",
      "[348]\ttrain-logloss:0.00586\teval-logloss:0.04699\n",
      "[349]\ttrain-logloss:0.00585\teval-logloss:0.04692\n",
      "[350]\ttrain-logloss:0.00584\teval-logloss:0.04677\n",
      "[351]\ttrain-logloss:0.00584\teval-logloss:0.04680\n",
      "[352]\ttrain-logloss:0.00583\teval-logloss:0.04686\n",
      "[353]\ttrain-logloss:0.00582\teval-logloss:0.04681\n",
      "[354]\ttrain-logloss:0.00582\teval-logloss:0.04686\n",
      "[355]\ttrain-logloss:0.00581\teval-logloss:0.04697\n",
      "[356]\ttrain-logloss:0.00580\teval-logloss:0.04689\n",
      "[357]\ttrain-logloss:0.00580\teval-logloss:0.04692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[358]\ttrain-logloss:0.00579\teval-logloss:0.04677\n",
      "[359]\ttrain-logloss:0.00578\teval-logloss:0.04682\n",
      "[360]\ttrain-logloss:0.00578\teval-logloss:0.04678\n",
      "[361]\ttrain-logloss:0.00577\teval-logloss:0.04682\n",
      "[362]\ttrain-logloss:0.00577\teval-logloss:0.04665\n",
      "[363]\ttrain-logloss:0.00576\teval-logloss:0.04669\n",
      "[364]\ttrain-logloss:0.00575\teval-logloss:0.04662\n",
      "[365]\ttrain-logloss:0.00575\teval-logloss:0.04668\n",
      "[366]\ttrain-logloss:0.00574\teval-logloss:0.04651\n",
      "[367]\ttrain-logloss:0.00573\teval-logloss:0.04655\n",
      "[368]\ttrain-logloss:0.00573\teval-logloss:0.04659\n",
      "[369]\ttrain-logloss:0.00572\teval-logloss:0.04663\n",
      "[370]\ttrain-logloss:0.00572\teval-logloss:0.04648\n",
      "[371]\ttrain-logloss:0.00571\teval-logloss:0.04632\n",
      "[372]\ttrain-logloss:0.00570\teval-logloss:0.04643\n",
      "[373]\ttrain-logloss:0.00570\teval-logloss:0.04640\n",
      "[374]\ttrain-logloss:0.00569\teval-logloss:0.04644\n",
      "[375]\ttrain-logloss:0.00569\teval-logloss:0.04653\n",
      "[376]\ttrain-logloss:0.00568\teval-logloss:0.04656\n",
      "[377]\ttrain-logloss:0.00567\teval-logloss:0.04660\n",
      "[378]\ttrain-logloss:0.00567\teval-logloss:0.04643\n",
      "[379]\ttrain-logloss:0.00566\teval-logloss:0.04629\n",
      "[380]\ttrain-logloss:0.00566\teval-logloss:0.04633\n",
      "[381]\ttrain-logloss:0.00565\teval-logloss:0.04626\n",
      "[382]\ttrain-logloss:0.00564\teval-logloss:0.04637\n",
      "[383]\ttrain-logloss:0.00564\teval-logloss:0.04641\n",
      "[384]\ttrain-logloss:0.00563\teval-logloss:0.04620\n",
      "[385]\ttrain-logloss:0.00563\teval-logloss:0.04627\n",
      "[386]\ttrain-logloss:0.00562\teval-logloss:0.04611\n",
      "[387]\ttrain-logloss:0.00562\teval-logloss:0.04615\n",
      "[388]\ttrain-logloss:0.00561\teval-logloss:0.04620\n",
      "[389]\ttrain-logloss:0.00560\teval-logloss:0.04617\n",
      "[390]\ttrain-logloss:0.00560\teval-logloss:0.04621\n",
      "[391]\ttrain-logloss:0.00559\teval-logloss:0.04605\n",
      "[392]\ttrain-logloss:0.00559\teval-logloss:0.04609\n",
      "[393]\ttrain-logloss:0.00558\teval-logloss:0.04613\n",
      "[394]\ttrain-logloss:0.00558\teval-logloss:0.04592\n",
      "[395]\ttrain-logloss:0.00557\teval-logloss:0.04589\n",
      "[396]\ttrain-logloss:0.00556\teval-logloss:0.04599\n",
      "[397]\ttrain-logloss:0.00556\teval-logloss:0.04606\n",
      "[398]\ttrain-logloss:0.00555\teval-logloss:0.04591\n",
      "[399]\ttrain-logloss:0.00555\teval-logloss:0.04596\n"
     ]
    }
   ],
   "source": [
    "#train()로 학습을 하자. \n",
    "params = {  'max_depth':3,        # 트리의 최대 깊이\n",
    "            'eta' : 0.1,          # 학습율\n",
    "            'objective':'binary:logistic', # 2진 분류,3개 이상 분류는 multi:softmax\n",
    "            'eval_metric':'logloss',       # 손실함수\n",
    "            'early_stoppings' : 100 \n",
    "}\n",
    "num_round = 400 \n",
    "\n",
    "wlist = [(dtrain,'train'),(dtest,'eval')]\n",
    "xgb_model = xgb.train(params=params,dtrain=dtrain, num_boost_round=num_round,evals=wlist)\n",
    "# 딥러닝처럼 로그로스나 이런걸 눈으로 확인할 수 있는 반면 수작업이 많다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c55202b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "767dd972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function train in module xgboost.training:\n",
      "\n",
      "train(params: Dict[str, Any], dtrain: xgboost.core.DMatrix, num_boost_round: int = 10, *, evals: Optional[Sequence[Tuple[xgboost.core.DMatrix, str]]] = None, obj: Optional[Callable[[numpy.ndarray, xgboost.core.DMatrix], Tuple[numpy.ndarray, numpy.ndarray]]] = None, feval: Optional[Callable[[numpy.ndarray, xgboost.core.DMatrix], Tuple[str, float]]] = None, maximize: Optional[bool] = None, early_stopping_rounds: Optional[int] = None, evals_result: Optional[Dict[str, Dict[str, Union[List[float], List[Tuple[float, float]]]]]] = None, verbose_eval: Union[bool, int, NoneType] = True, xgb_model: Union[str, os.PathLike, xgboost.core.Booster, bytearray, NoneType] = None, callbacks: Optional[Sequence[xgboost.callback.TrainingCallback]] = None, custom_metric: Optional[Callable[[numpy.ndarray, xgboost.core.DMatrix], Tuple[str, float]]] = None) -> xgboost.core.Booster\n",
      "    Train a booster with given parameters.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    params :\n",
      "        Booster params.\n",
      "    dtrain :\n",
      "        Data to be trained.\n",
      "    num_boost_round :\n",
      "        Number of boosting iterations.\n",
      "    evals :\n",
      "        List of validation sets for which metrics will evaluated during training.\n",
      "        Validation metrics will help us track the performance of the model.\n",
      "    obj\n",
      "        Custom objective function.  See :doc:`Custom Objective\n",
      "        </tutorials/custom_metric_obj>` for details.\n",
      "    feval :\n",
      "        .. deprecated:: 1.6.0\n",
      "            Use `custom_metric` instead.\n",
      "    maximize :\n",
      "        Whether to maximize feval.\n",
      "    early_stopping_rounds :\n",
      "        Activates early stopping. Validation metric needs to improve at least once in\n",
      "        every **early_stopping_rounds** round(s) to continue training.\n",
      "        Requires at least one item in **evals**.\n",
      "        The method returns the model from the last iteration (not the best one).  Use\n",
      "        custom callback or model slicing if the best model is desired.\n",
      "        If there's more than one item in **evals**, the last entry will be used for early\n",
      "        stopping.\n",
      "        If there's more than one metric in the **eval_metric** parameter given in\n",
      "        **params**, the last metric will be used for early stopping.\n",
      "        If early stopping occurs, the model will have two additional fields:\n",
      "        ``bst.best_score``, ``bst.best_iteration``.\n",
      "    evals_result :\n",
      "        This dictionary stores the evaluation results of all the items in watchlist.\n",
      "    \n",
      "        Example: with a watchlist containing\n",
      "        ``[(dtest,'eval'), (dtrain,'train')]`` and\n",
      "        a parameter containing ``('eval_metric': 'logloss')``,\n",
      "        the **evals_result** returns\n",
      "    \n",
      "        .. code-block:: python\n",
      "    \n",
      "            {'train': {'logloss': ['0.48253', '0.35953']},\n",
      "             'eval': {'logloss': ['0.480385', '0.357756']}}\n",
      "    \n",
      "    verbose_eval :\n",
      "        Requires at least one item in **evals**.\n",
      "        If **verbose_eval** is True then the evaluation metric on the validation set is\n",
      "        printed at each boosting stage.\n",
      "        If **verbose_eval** is an integer then the evaluation metric on the validation set\n",
      "        is printed at every given **verbose_eval** boosting stage. The last boosting stage\n",
      "        / the boosting stage found by using **early_stopping_rounds** is also printed.\n",
      "        Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric\n",
      "        is printed every 4 boosting stages, instead of every boosting stage.\n",
      "    xgb_model :\n",
      "        Xgb model to be loaded before training (allows training continuation).\n",
      "    callbacks :\n",
      "        List of callback functions that are applied at end of each iteration.\n",
      "        It is possible to use predefined callbacks by using\n",
      "        :ref:`Callback API <callback_api>`.\n",
      "    \n",
      "        .. note::\n",
      "    \n",
      "           States in callback are not preserved during training, which means callback\n",
      "           objects can not be reused for multiple training sessions without\n",
      "           reinitialization or deepcopy.\n",
      "    \n",
      "        .. code-block:: python\n",
      "    \n",
      "            for params in parameters_grid:\n",
      "                # be sure to (re)initialize the callbacks before each run\n",
      "                callbacks = [xgb.callback.LearningRateScheduler(custom_rates)]\n",
      "                xgboost.train(params, Xy, callbacks=callbacks)\n",
      "    \n",
      "    custom_metric:\n",
      "    \n",
      "        .. versionadded 1.6.0\n",
      "    \n",
      "        Custom metric function.  See :doc:`Custom Metric </tutorials/custom_metric_obj>`\n",
      "        for details.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    Booster : a trained booster model\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(help(xgb.train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4ffdeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
